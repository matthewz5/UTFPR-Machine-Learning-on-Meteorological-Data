{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Research for the statistical analysis of meteorological data and machine learning methods for determining solar irradiation.\n",
    "\n",
    "Researcher: Matheus Henrique da Silva.\n",
    "Control and Automation Engineering Student.\n",
    "Universidade Tecnológica Federal do Paraná, Cornélio Procópio, Paraná, Brasil. \n",
    "E-mail: matheussilva.2019@alunos.utfpr.edu.br. \n",
    "ID Lattes: 5450995625966991.\n",
    "\n",
    "Supervisor: Wesley Angelino de Souza.\n",
    "Lecturer in the Graduate Program in Electrical Engineering.\n",
    "Universidade Tecnológica Federal do Paraná, Cornélio Procópio, Paraná, Brasil. \n",
    "E-mail: wesleyangelino@utfpr.edu.br. \n",
    "ID Lattes: 8594457321079718."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "#### Step 1 - data extraction and agglutination\n",
    "\n",
    "Data collected and made available by the Instituto Nacional de Meteorologia (INMET), acess on https://portal.inmet.gov.br/, in format .csv of 606 meteorological collection stations of Brazil between 2010 and 2021.\n",
    "\n",
    "Each .csv file contains 9 lines with information about the station: Nome; Codigo Estacao; Latitude; Longitude; Altitude; Situacao; Data Inicial; Data Final e Periodicidade da Medicao.\n",
    "\n",
    "In line 11, contains the name of the variables: Data Medicao; Hora Medicao; PRECIPITACAO TOTAL, HORARIO(mm); PRESSAO ATMOSFERICA AO NIVEL DA ESTACAO, HORARIA(mB); PRESSAO ATMOSFERICA REDUZIDA NIVEL DO MAR, AUT(mB); PRESSAO ATMOSFERICA MAX.NA HORA ANT. (AUT)(mB); PRESSAO ATMOSFERICA MIN. NA HORA ANT. (AUT)(mB); RADIACAO GLOBAL(Kj/mÂ²); TEMPERATURA DA CPU DA ESTACAO(Â°C); TEMPERATURA DO AR - BULBO SECO, HORARIA(Â°C); TEMPERATURA DO PONTO DE ORVALHO(Â°C); TEMPERATURA MAXIMA NA HORA ANT. (AUT)(Â°C); TEMPERATURA MINIMA NA HORA ANT. (AUT)(Â°C); TEMPERATURA ORVALHO MAX. NA HORA ANT. (AUT)(Â°C); TEMPERATURA ORVALHO MIN. NA HORA ANT. (AUT)(Â°C); TENSAO DA BATERIA DA ESTACAO(V); UMIDADE REL. MAX. NA HORA ANT. (AUT)(%); UMIDADE REL. MIN. NA HORA ANT. (AUT)(%); UMIDADE RELATIVA DO AR, HORARIA(%); VENTO, DIRECAO HORARIA (gr)(Â° (gr)); VENTO, RAJADA MAXIMA(m/s); VENTO, VELOCIDADE HORARIA(m/s).\n",
    "\n",
    "From line 12, the data collected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe with the informations of all stations\n",
    "\n",
    "df_stations = pd.DataFrame(columns =  [\"csv\", \"nome\", \"codigo\", \"latitude\", \n",
    "                                       \"longitude\", \"altitude\", \"situacao\", \"data_inicio\", \n",
    "                                       \"data_fim\", \"periodicidade\"])\n",
    "\n",
    "for file in glob.glob(\"*.csv\"):\n",
    "        \n",
    "    arquivo = file\n",
    "    file = open(file)\n",
    "\n",
    "    csv_reader = csv.reader(file)\n",
    "\n",
    "    # 1: 'nome': nome_estacao\n",
    "    row = next(csv_reader)\n",
    "    row=str(row[0])\n",
    "    tmp=row.split(\": \")\n",
    "    nome_estacao = tmp[1]\n",
    "    \n",
    "    # 2: 'codigo': codigo_estacao\n",
    "    row = next(csv_reader)\n",
    "    row=str(row[0])\n",
    "    tmp=row.split(\": \")\n",
    "    codigo_estacao = tmp[1]\n",
    "    \n",
    "    # 3: 'latitude': latitude_estacao\n",
    "    row = next(csv_reader)\n",
    "    row=str(row[0])\n",
    "    tmp=row.split(\": \")\n",
    "    latitude_estacao = tmp[1]\n",
    "    \n",
    "    # 4: 'longitude': longitude_estacao\n",
    "    row = next(csv_reader)\n",
    "    row=str(row[0])\n",
    "    tmp=row.split(\": \")\n",
    "    longitude_estacao = tmp[1]\n",
    "    \n",
    "    # 5: 'altitude': altitude_estacao\n",
    "    row = next(csv_reader)\n",
    "    row=str(row[0])\n",
    "    tmp=row.split(\": \")\n",
    "    altitude_estacao = tmp[1]\n",
    "    \n",
    "    # 5: 'situacao': situacao_estacao\n",
    "    row = next(csv_reader)\n",
    "    row=str(row[0])\n",
    "    tmp=row.split(\": \")\n",
    "    situacao_estacao = tmp[1]\n",
    "    \n",
    "    # 6: 'data_inicio': data_inicio_coleta_estacao\n",
    "    row = next(csv_reader)\n",
    "    row=str(row[0])\n",
    "    tmp=row.split(\": \")\n",
    "    data_inicio_coleta_estacao = tmp[1]\n",
    "    \n",
    "    # 7: 'data_fim': data_fim_coleta_estacao\n",
    "    row = next(csv_reader)\n",
    "    row=str(row[0])\n",
    "    tmp=row.split(\": \")\n",
    "    data_fim_coleta_estacao = tmp[1]\n",
    "\n",
    "    # 8: 'periodicidade': peridiocidade_dados_estacao\n",
    "    row = next(csv_reader)\n",
    "    row=str(row[0])\n",
    "    tmp=row.split(\": \")\n",
    "    peridiocidade_dados_estacao = tmp[1]\n",
    "    \n",
    "    new_row = {'csv': str(arquivo), \n",
    "               'nome': nome_estacao, \n",
    "               'codigo': codigo_estacao, \n",
    "               'latitude': latitude_estacao,\n",
    "               'longitude': longitude_estacao,\n",
    "               'altitude': altitude_estacao,\n",
    "               'situacao': situacao_estacao, \n",
    "               'data_inicio': data_inicio_coleta_estacao, \n",
    "               'data_fim': data_fim_coleta_estacao, \n",
    "               'periodicidade': peridiocidade_dados_estacao}\n",
    "    \n",
    "    df_stations = df_stations.append(new_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identification of data types and changes for station selection\n",
    "\n",
    "data_types = df_stations.dtypes\n",
    "\n",
    "print(data_types)\n",
    "\n",
    "df_stations['latitude'] = df_stations['latitude'].astype(float)\n",
    "df_stations['longitude'] = df_stations['longitude'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selection of the stations in Cornélio Procópio-PR region\n",
    "\n",
    "lat = -23.185038698153438\n",
    "lon = -50.647548006591066\n",
    "dist = 4\n",
    "\n",
    "lim_lat_higher = lat + dist \n",
    "lim_lat_bottom = lat - dist\n",
    "\n",
    "lon =  -50.647548006591066\n",
    "lim_lon_right = lon + dist\n",
    "lim_lon_left = lon - dist\n",
    "\n",
    "selected_stations = df_stations[((df_stations['latitude'] >= lim_lat_bottom) & (df_stations['latitude'] <= lim_lat_higher)) & \\\n",
    "                            ((df_stations['longitude'] >= lim_lon_left) & (df_stations['longitude'] <= lim_lon_right))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_stations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic map for station location\n",
    "\n",
    "map = folium.Map(location=[lat, lon], zoom_start=14)\n",
    "\n",
    "folium.Marker(location=[lat, lon], popup='CORNÉLIO PROCÓPIO').add_to(map)\n",
    "\n",
    "lenght = selected_stations.shape[0]\n",
    "\n",
    "lat_ar = np.zeros(lenght)\n",
    "lon_ar = np.zeros(lenght)\n",
    "est_ar = np.empty((lenght), dtype=object)\n",
    "\n",
    "index = selected_stations.index.to_numpy()\n",
    "\n",
    "for i in range(lenght):\n",
    "\n",
    "    lat_ar[i] = selected_stations.loc[index[i], 'latitude']\n",
    "    lon_ar[i] = selected_stations.loc[index[i], 'longitude']\n",
    "    est_ar[i] = selected_stations.loc[index[i], 'nome']\n",
    "\n",
    "    popup_station = folium.Popup(f'{est_ar[i]}\\n{lat_ar[i]},{lon_ar[i]}')\n",
    "    folium.Marker(location=[lat_ar[i], lon_ar[i]], popup=popup_station).add_to(map)\n",
    "\n",
    "map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe with the data agglutination\n",
    "\n",
    "data_loaded = 1\n",
    "\n",
    "if(data_loaded==0):\n",
    "    \n",
    "    result = pd.DataFrame()\n",
    "\n",
    "    for file in selected_stations['csv']:\n",
    "\n",
    "        df_temp = pd.read_csv(file, skiprows=10, delimiter=\";\", decimal=\",\")\n",
    "        df_temp.drop(df_temp.columns[[4,5,6,8,11,12,13,14,15,16,17,22]],axis=1, inplace = True)\n",
    "        df_temp.dropna(inplace=True)\n",
    "\n",
    "        result = pd.concat([result, df_temp])\n",
    "        \n",
    "    result.to_csv('data_agglutinated/data_selected_stations.csv') \n",
    "    \n",
    "else:\n",
    "    result = pd.read_csv('data_agglutinated/data_selected_stations.csv', delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "#### Step 2 - statistical and exploratory analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.1 - load and transformation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "data = pd.read_csv('data_agglutinated/data_selected_stations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change variables name and format of date\n",
    "\n",
    "data.drop(data.columns[0],axis=1, inplace = True)\n",
    "data.rename(columns = {'Data Medicao': 'data'}, inplace = True)\n",
    "data['data'] = pd.to_datetime(data['data'])\n",
    "data.rename(columns = {'Hora Medicao': 'hora'}, inplace = True)\n",
    "data.rename(columns = {'PRECIPITACAO TOTAL, HORARIO(mm)': 'precipitacao'}, inplace = True)\n",
    "data.rename(columns = {'PRESSAO ATMOSFERICA AO NIVEL DA ESTACAO, HORARIA(mB)': 'pressao_atm'}, inplace = True)\n",
    "data.rename(columns = {'RADIACAO GLOBAL(Kj/m²)': 'radiacao'}, inplace = True)\n",
    "data.rename(columns = {'TEMPERATURA DO AR - BULBO SECO, HORARIA(°C)': 'temperatura'}, inplace = True)\n",
    "data.rename(columns = {'TEMPERATURA DO PONTO DE ORVALHO(°C)': 'temp_orvalho'}, inplace = True)\n",
    "data.rename(columns = {'UMIDADE RELATIVA DO AR, HORARIA(%)': 'umidade'}, inplace = True)\n",
    "data.rename(columns = {'VENTO, DIRECAO HORARIA (gr)(° (gr))': 'direcao_vento'}, inplace = True)\n",
    "data.rename(columns = {'VENTO, RAJADA MAXIMA(m/s)': 'vento_maximo'}, inplace = True)\n",
    "data.rename(columns = {'VENTO, VELOCIDADE HORARIA(m/s)': 'velocidade'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of target variable 'radiacao' by 'hora'\n",
    "\n",
    "data.plot(kind=\"scatter\", x=\"hora\", y=\"radiacao\", alpha=0.1)\n",
    "\n",
    "plt.savefig('figures/scatter_radiacao_hora.png', format='png', dpi = 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the preliminary analyses, we noticed that the data are concentrated between 10 am and 10 pm\n",
    "\n",
    "However, solar radiation begins to be noticed along with sunrise ~ 6:40 am\n",
    "\n",
    "Therefore, we will shift the data by -3 hours each (due to the Brazilian time zone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_hour(df):\n",
    "\n",
    "    df['hora'] = df['hora'].replace(0, 21)\n",
    "    df['hora'] = df['hora'].replace(100, 22)\n",
    "    df['hora'] = df['hora'].replace(200, 23)\n",
    "    df['hora'] = df['hora'].replace(300, 0)\n",
    "    df['hora'] = df['hora'].replace(400, 1)\n",
    "    df['hora'] = df['hora'].replace(500, 2)\n",
    "    df['hora'] = df['hora'].replace(600, 3)\n",
    "    df['hora'] = df['hora'].replace(700, 4)\n",
    "    df['hora'] = df['hora'].replace(800, 5)\n",
    "    df['hora'] = df['hora'].replace(900, 6)\n",
    "    df['hora'] = df['hora'].replace(1000, 7)\n",
    "    df['hora'] = df['hora'].replace(1100, 8)\n",
    "    df['hora'] = df['hora'].replace(1200, 9)\n",
    "    df['hora'] = df['hora'].replace(1300, 10)\n",
    "    df['hora'] = df['hora'].replace(1400, 11)\n",
    "    df['hora'] = df['hora'].replace(1500, 12)\n",
    "    df['hora'] = df['hora'].replace(1600, 13)\n",
    "    df['hora'] = df['hora'].replace(1700, 14)\n",
    "    df['hora'] = df['hora'].replace(1800, 15)\n",
    "    df['hora'] = df['hora'].replace(1900, 16)\n",
    "    df['hora'] = df['hora'].replace(2000, 17)\n",
    "    df['hora'] = df['hora'].replace(2100, 18)\n",
    "    df['hora'] = df['hora'].replace(2200, 19)\n",
    "    df['hora'] = df['hora'].replace(2300, 20)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = adjust_hour(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data dataframe info, quantities and types\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for NULL values\n",
    "\n",
    "non_null_counts = data.count()\n",
    "\n",
    "print(non_null_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical describe\n",
    "\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.2 - data cleaning\n",
    "\n",
    "Previously, there were discrepant values ​​noticed at unconventional times, for this reason they caused the outliers to be cleaned in a different way:\n",
    "\n",
    "We take the data per hour, and remove according to the variable_target 'radiacao' the last 3.5 * standard deviation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_out(df):\n",
    "\n",
    "    dados_0 = df[df['hora'] == 0]\n",
    "    dados_1 = df[df['hora'] == 1]\n",
    "    dados_2 = df[df['hora'] == 2]\n",
    "    dados_3 = df[df['hora'] == 3]\n",
    "    dados_4 = df[df['hora'] == 4]\n",
    "    dados_5 = df[df['hora'] == 5]\n",
    "    dados_6 = df[df['hora'] == 6]\n",
    "    dados_7 = df[df['hora'] == 7]\n",
    "    dados_8 = df[df['hora'] == 8]\n",
    "    dados_9 = df[df['hora'] == 9]\n",
    "    dados_10 = df[df['hora'] == 10]\n",
    "    dados_11 = df[df['hora'] == 11]\n",
    "    dados_12 = df[df['hora'] == 12]\n",
    "    dados_13 = df[df['hora'] == 13]\n",
    "    dados_14 = df[df['hora'] == 14]\n",
    "    dados_15 = df[df['hora'] == 15]\n",
    "    dados_16 = df[df['hora'] == 16]\n",
    "    dados_17 = df[df['hora'] == 17]\n",
    "    dados_18 = df[df['hora'] == 18]\n",
    "    dados_19 = df[df['hora'] == 19]\n",
    "    dados_20 = df[df['hora'] == 20]\n",
    "    dados_21 = df[df['hora'] == 21]\n",
    "    dados_22 = df[df['hora'] == 22]\n",
    "    dados_23 = df[df['hora'] == 23]\n",
    "\n",
    "    data_hour = [dados_0, dados_1, dados_2, dados_3, dados_4, dados_5, dados_6, dados_7, dados_8, dados_9, dados_10, dados_11, dados_12, dados_13, dados_14, dados_15, dados_16, dados_17, dados_18, dados_19, dados_20, dados_21, dados_22, dados_23]\n",
    "\n",
    "    data_filtered = pd.DataFrame()\n",
    "\n",
    "    for i in data_hour:\n",
    "\n",
    "        mean_radiacao = np.mean(i['radiacao'])\n",
    "        std_radiacao = np.std(i['radiacao'])\n",
    "\n",
    "        lower = mean_radiacao - 3.5 * std_radiacao\n",
    "        upper = mean_radiacao + 3.5 * std_radiacao\n",
    "\n",
    "        y = i.query('radiacao <= @upper')\n",
    "\n",
    "        data_filtered = pd.concat([data_filtered, y])\n",
    "\n",
    "    return data_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = remove_out(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of target variable 'radiacao' by 'hora' with filtered data\n",
    "\n",
    "data.plot(kind=\"scatter\", x=\"hora\", y=\"radiacao\", alpha=0.1)\n",
    "\n",
    "plt.savefig('figures/scatter_radiacao_hora_filtered.png', format='png', dpi = 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.3 - statistical analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical describe with filtered data\n",
    "\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of the variables\n",
    "\n",
    "def histogram_variables(df, filename=None):\n",
    "\n",
    "    df.hist(bins=50, figsize=(20,15))\n",
    "\n",
    "    if filename is not None:\n",
    "        plt.savefig(filename, format='pdf', bbox_inches='tight')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_variables(data, 'figures/histogram_variables.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter of the target variable 'radiacao' with all variables\n",
    "\n",
    "def scatter_variables(df, variable, filename=None):\n",
    "\n",
    "    variables = df.columns.drop(variable)\n",
    "\n",
    "    data = df[[variable] + list(variables)]\n",
    "\n",
    "    sns.pairplot(data, x_vars=variables, y_vars=variable, height=2.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if filename is not None:\n",
    "        plt.savefig(filename, dpi=500, bbox_inches='tight')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_variables(data, 'radiacao', 'figures/scatter_variables.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skewness values\n",
    "\n",
    "data.skew(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pearson correlation\n",
    "\n",
    "def pearson_correlation_map(df, filename=None):\n",
    "\n",
    "    plt.figure(figsize = (12,8))\n",
    "\n",
    "    sns.heatmap(df.corr(), annot = True, fmt = '.4f', cmap = 'Reds', vmax = .99, vmin = -0.60)\n",
    "\n",
    "    if filename is not None:\n",
    "        plt.savefig(filename, format='pdf', bbox_inches='tight')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pearson correlation plot\n",
    "\n",
    "pearson_correlation_map(data, 'figures/pearson_correlation_map.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check multicollinearity problem\n",
    "\n",
    "df_correlation = data.corr()\n",
    "\n",
    "df_correlation = df_correlation[((df_correlation >= .65) | (dfCorr <= -.65)) & (df_correlation !=1.000)]\n",
    "df_correlation = df_correlation.drop('radiacao', axis=1)\n",
    "df_correlation = df_correlation.drop('radiacao', axis=0)\n",
    "\n",
    "df_correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As there is a high correlation between 'vento_maximo' and 'velocidade' \n",
    "# let's remove 'velocidade' from the analysis, because it has the lowest correlation with 'radiacao'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "#### Step 3 - data normalization and division\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3.1 - normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StardartScaler\n",
    "\n",
    "def norm_std(df, variables):\n",
    "\n",
    "    scaler_lab = StandardScaler()\n",
    "    scaler_lab.fit(df[variables])\n",
    "    scaled_features_lab = scaler_lab.transform(df[variables])\n",
    "\n",
    "    data_std = pd.DataFrame(scaled_features_lab, columns = variables, index = df.index)\n",
    "\n",
    "    return data_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MinMaxScaler\n",
    "\n",
    "def norm_minmax(df, variables):\n",
    "\n",
    "    data_minmax = df.copy()\n",
    "\n",
    "    scaler_lab = MinMaxScaler()\n",
    "\n",
    "    data_minmax[variables] = scaler_lab.fit_transform(data_minmax[variables])\n",
    "\n",
    "    return data_minmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a data backup, with the variable 'data'\n",
    "\n",
    "backup_data = pd.DataFrame({'radiacao': data['radiacao'],\n",
    "                            'data': data['data'],\n",
    "                            'hora': data['hora'],\n",
    "                            'precipitacao': data['precipitacao'],\n",
    "                            'pressao_atm': data['pressao_atm'],\n",
    "                            'temperatura': data['temperatura'],\n",
    "                            'temp_orvalho': data['temp_orvalho'],\n",
    "                            'umidade': data['umidade'],\n",
    "                            'direcao_vento': data['direcao_vento'],\n",
    "                            'vento_maximo': data['vento_maximo']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe wiithou the variable 'data' beacause we keep this to the future steps of testins\n",
    "\n",
    "data2 = pd.DataFrame({'radiacao': data['radiacao'],\n",
    "                    'hora': data['hora'],\n",
    "                    'precipitacao': data['precipitacao'],\n",
    "                    'pressao_atm': data['pressao_atm'],\n",
    "                    'temperatura': data['temperatura'],\n",
    "                    'temp_orvalho': data['temp_orvalho'],\n",
    "                    'umidade': data['umidade'],\n",
    "                    'direcao_vento': data['direcao_vento'],\n",
    "                    'vento_maximo': data['vento_maximo']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables list\n",
    "\n",
    "variables = ['radiacao', 'hora', 'precipitacao', 'pressao_atm', 'temperatura', 'temp_orvalho', 'umidade', 'direcao_vento', 'vento_maximo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot figure to the variables of the data\n",
    "\n",
    "def boxplots_variables(df, filename=None):\n",
    "\n",
    "    melted_df = df.melt(value_vars=df, var_name='Variable', value_name='Values')\n",
    "\n",
    "    sns.catplot(data=melted_df, x='Variable', y='Values', kind='box', height=8, aspect=1.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if filename is not None:\n",
    "        plt.savefig(filename, dpi=500, bbox_inches='tight')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STD normalization and bloxplot\n",
    "\n",
    "data_std = norm_std(data2, variables)\n",
    "\n",
    "boxplots_variables(data_std, 'figures/bloxplot_data_std.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MINMAX normalization and boxplot\n",
    "\n",
    "data_minmax = norm_minmax(data2, variables)\n",
    "\n",
    "boxplots_variables(data_minmax, 'figures/bloxplot_data_minmax.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3.2 - data division\n",
    "\n",
    "70% -> train\n",
    "\n",
    "20% -> test\n",
    "\n",
    "10% -> validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_divide(df):\n",
    "\n",
    "    X = df[['hora', 'precipitacao', 'pressao_atm', 'temperatura', 'temp_orvalho', 'umidade', 'direcao_vento', 'vento_maximo']].values\n",
    "    y = df.radiacao.values.reshape(-1,1)\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.125, random_state=42)\n",
    "\n",
    "    return x_train, y_train, x_test, y_test, x_valid, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StandartScaler\n",
    "\n",
    "x_train_std, y_train_std, x_test_std, y_test_std, x_valid_std, y_valid_std = data_divide(data_std)\n",
    "\n",
    "# Checking division\n",
    "\n",
    "print('train 70% =', len(y_train_std))\n",
    "print('test 20%  =', len(y_test_std))\n",
    "print('validation 10% =', len(y_valid_std))\n",
    "\n",
    "print(len(data_std), '=', len(y_train_std)+len(y_test_std)+len(y_valid_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MinMaxScaler\n",
    "\n",
    "x_train_minmax, y_train_minmax, x_test_minmax, y_test_minmax, x_valid_minmax, y_valid_minmax = data_divide(data_minmax)\n",
    "\n",
    "# Checking division\n",
    "\n",
    "print('train 70% =', len(y_train_minmax))\n",
    "print('test 20%  =', len(y_test_minmax))\n",
    "print('validation 10% =', len(y_valid_minmax))\n",
    "\n",
    "print(len(data_std), '=', len(y_train_minmax)+len(y_test_minmax)+len(y_valid_minmax))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "#### Step 4 - Machine Learning models training\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 4.1 - Multilayer Perceptron Regressor (MLPR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "\n",
    "def model_mlpr(x_train, y_train):\n",
    "\n",
    "    model = MLPRegressor(hidden_layer_sizes=(100, 50), activation='relu', solver='adam', random_state=42)\n",
    "    \n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing, error calculator and dataframe function\n",
    "\n",
    "def model_mlpr_test(model, x_test, y_test):\n",
    "\n",
    "    y_pred_mlpr = model.predict(x_test)\n",
    "\n",
    "    MAE = metrics.mean_absolute_error(y_test, y_pred_mlpr)\n",
    "    MSE = metrics.mean_squared_error(y_test, y_pred_mlpr)\n",
    "    RMSE = np.sqrt(metrics.mean_squared_error(y_test, y_pred_mlpr))\n",
    "    R2 = metrics.r2_score(y_test, y_pred_mlpr)\n",
    "\n",
    "    print('-'*60)\n",
    "    print('Mean Absolute Error:', MAE)\n",
    "    print('Mean Squared Error:', MSE)\n",
    "    print('Root Mean Squared Error:', RMSE)\n",
    "    print('R2 Score:', R2)\n",
    "\n",
    "    y_test = np.array(y_test)\n",
    "    y_pred_mlpr = np.array(y_pred_mlpr)\n",
    "\n",
    "    df_determined = pd.DataFrame({'measured_rad': y_test.flatten(), 'determined_rad': y_pred_mlpr.flatten()})\n",
    "\n",
    "    print('-'*60)\n",
    "    print(df_determined)\n",
    "    print('-'*60)\n",
    "\n",
    "    return df_determined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STD MLPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_mlpr_std = 0\n",
    "\n",
    "if control_mlpr_std == 1:\n",
    "\n",
    "    model_mlpr_std = model_mlpr(x_train_std, y_train_std)\n",
    "\n",
    "    joblib.dump(model_mlpr_std, 'ml_models/model_mlpr_std.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_mlpr_std_test = 0\n",
    "\n",
    "if control_mlpr_std_test == 1:\n",
    "\n",
    "    df_determined_mlpr_std = model_mlpr_test(model_mlpr_std, x_test_std, y_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MINMAX MLPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_mlpr_minmax = 0\n",
    "\n",
    "if control_mlpr_minmax == 1:\n",
    "\n",
    "    model_mlpr_minmax = model_mlpr(x_train_minmax, y_train_minmax)\n",
    "\n",
    "    joblib.dump(model_mlpr_minmax, 'ml_models/modelo_mlpr_minmax.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_mlpr_minmax_test = 0\n",
    "\n",
    "if control_mlpr_minmax_test == 1:\n",
    "\n",
    "    df_determined_mlpr_minmax = model_mlpr_test(model_mlpr_minmax, x_test_minmax, y_test_minmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 4.2 - Decision Tree Regressor (DTR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "\n",
    "def model_dtr(x_train, y_train):\n",
    "\n",
    "    model = DecisionTreeRegressor(max_depth=100, min_samples_leaf=2, random_state = 42)\n",
    "\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing, error calculator and dataframe function\n",
    "\n",
    "def model_dtr_test(model, x_test, y_test):\n",
    "\n",
    "    y_pred_dtr = model.predict(x_test)\n",
    "\n",
    "    MAE = metrics.mean_absolute_error(y_test, y_pred_dtr)\n",
    "    MSE = metrics.mean_squared_error(y_test, y_pred_dtr)\n",
    "    RMSE = np.sqrt(metrics.mean_squared_error(y_test, y_pred_dtr))\n",
    "    R2 = metrics.r2_score(y_test, y_pred_dtr)\n",
    "\n",
    "    print('-'*60)\n",
    "    print('Mean Absolute Error:', MAE)\n",
    "    print('Mean Squared Error:', MSE)\n",
    "    print('Root Mean Squared Error:', RMSE)\n",
    "    print('R2 Score:', R2)\n",
    "    \n",
    "    y_test = np.array(y_test)\n",
    "    y_pred_dtr = np.array(y_pred_dtr)\n",
    "\n",
    "    df_determined = pd.DataFrame({'measured_rad': y_test.flatten(), 'determined_rad': y_pred_dtr.flatten()})\n",
    "\n",
    "    print('-'*60)\n",
    "    print(df_determined)\n",
    "    print('-'*60)\n",
    "\n",
    "    return df_determined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STD DTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_dtr_std = 0\n",
    "\n",
    "if control_dtr_std == 1:\n",
    "\n",
    "    model_dtr_std = model_dtr(x_train_std, y_train_std)\n",
    "    \n",
    "    joblib.dump(model_dtr_std, 'ml_models/model_dtr_std.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_dtr_std_test = 0\n",
    "\n",
    "if control_dtr_std_test == 1:\n",
    "    \n",
    "    df_determined_dtr_std = model_dtr_test(model_dtr_std, x_test_std, y_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MINMAX DTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_dtr_minmax = 0\n",
    "\n",
    "if control_dtr_minmax == 1:\n",
    "\n",
    "    model_dtr_minmax = model_dtr(x_train_minmax, y_train_minmax)\n",
    "    \n",
    "    joblib.dump(model_dtr_minmax, 'ml_models/model_dtr_minmax.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_dtr_minmax_test = 0\n",
    "\n",
    "if control_dtr_minmax_test == 1:\n",
    "\n",
    "    df_determined_dtr_minmax = model_dtr_test(model_dtr_minmax, x_test_minmax, y_test_minmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 4.3 - Random Forest Regressor (RFR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "\n",
    "def model_rfr(x_train, y_train):\n",
    "\n",
    "    model = RandomForestRegressor(n_estimators=100, max_depth=50, min_samples_leaf=2, random_state = 42)\n",
    "\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing, error calculator and dataframe function\n",
    "\n",
    "def model_rfr_test(model, x_test, y_test):\n",
    "    \n",
    "    y_pred_rfr = model.predict(x_test)\n",
    "\n",
    "    MAE = metrics.mean_absolute_error(y_test, y_pred_rfr)\n",
    "    MSE = metrics.mean_squared_error(y_test, y_pred_rfr)\n",
    "    RMSE = np.sqrt(metrics.mean_squared_error(y_test, y_pred_rfr))\n",
    "    R2 = metrics.r2_score(y_test, y_pred_rfr)\n",
    "\n",
    "    print('-'*60)\n",
    "    print('Mean Absolute Error:', MAE)\n",
    "    print('Mean Squared Error:', MSE)\n",
    "    print('Root Mean Squared Error:', RMSE)\n",
    "    print('R2 Score:', R2)\n",
    "    \n",
    "    y_test = np.array(y_test)\n",
    "    y_pred_rfr = np.array(y_pred_rfr)\n",
    "\n",
    "    df_determined = pd.DataFrame({'measured_rad': y_test.flatten(), 'determined_rad': y_pred_rfr.flatten()})\n",
    "\n",
    "    print('-'*60)\n",
    "    print(df_determined)\n",
    "    print('-'*60)\n",
    "\n",
    "    return df_determined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STD RFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_rfr_std = 0\n",
    "\n",
    "if control_rfr_std == 1:\n",
    "\n",
    "    model_rfr_std = model_rfr(x_train_std, y_train_std)\n",
    "    \n",
    "    joblib.dump(model_rfr_std, 'ml_models/model_rfr_std.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_rfr_std_test = 0\n",
    "\n",
    "if control_rfr_std_test == 1:\n",
    "    \n",
    "    df_determined_rfr_std = model_rfr_test(model_rfr_std, x_test_std, y_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MINMAX RFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_rfr_minmax = 1\n",
    "\n",
    "if control_rfr_minmax == 1:\n",
    "\n",
    "    model_rfr_minmax = model_rfr(x_train_minmax, y_train_minmax)\n",
    "\n",
    "    joblib.dump(control_rfr_minmax, 'ml_models/model_rfr_minmax.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_rfr_minmax_test = 1\n",
    "\n",
    "if control_rfr_minmax_test == 1:\n",
    "\n",
    "    df_determined_rfr_minmax = model_rfr_test(model_rfr_minmax, x_test_minmax, y_test_minmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 4.4 - K Nearest Neighbors Regressor (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "\n",
    "def model_knn(x_train, y_train):\n",
    "\n",
    "    model = KNeighborsRegressor(n_neighbors=3, leaf_size=30, weights='distance', algorithm='auto', metric='euclidean')\n",
    "    \n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing, error calculator and dataframe function\n",
    "\n",
    "def model_knn_test(model, x_test, y_test):\n",
    "\n",
    "    y_pred_knn = model.predict(x_test)\n",
    "\n",
    "    MAE = metrics.mean_absolute_error(y_test, y_pred_knn)\n",
    "    MSE = metrics.mean_squared_error(y_test, y_pred_knn)\n",
    "    RMSE = np.sqrt(metrics.mean_squared_error(y_test, y_pred_knn))\n",
    "    R2 = metrics.r2_score(y_test, y_pred_knn)\n",
    "\n",
    "    print('-'*60)\n",
    "    print('Mean Absolute Error:', MAE)\n",
    "    print('Mean Squared Error:', MSE)\n",
    "    print('Root Mean Squared Error:', RMSE)\n",
    "    print('R2 Score:', R2)\n",
    "    \n",
    "    y_test = np.array(y_test)\n",
    "    y_pred_knn = np.array(y_pred_knn)\n",
    "\n",
    "    df_determined = pd.DataFrame({'Valor_Real': y_test.flatten(), 'Valor_Previsto': y_pred_knn.flatten()})\n",
    "\n",
    "    print('-'*60)\n",
    "    print(df_determined)\n",
    "    print('-'*60)\n",
    "\n",
    "    return df_determined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STD KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_knn_std = 0\n",
    "\n",
    "if control_knn_std == 1:\n",
    "\n",
    "    model_knn_std = model_knn(x_train_std, y_train_std)\n",
    "    \n",
    "    joblib.dump(model_knn_std, 'ml_models/model_knn_std.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_knn_std_test = 0\n",
    "\n",
    "if control_knn_std_test == 1:\n",
    "    \n",
    "    df_determined_knn_std = model_knn_test(model_knn_std, x_test_std, y_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MINMAX KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_knn_minmax = 0\n",
    "\n",
    "if control_knn_minmax == 1:\n",
    "\n",
    "    model_knn_minmax = model_knn(x_train_minmax, y_train_minmax)\n",
    "\n",
    "    joblib.dump(model_knn_minmax, 'ml_models/model_knn_minmax.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_knn_minmax_test = 0\n",
    "\n",
    "if control_knn_minmax_test == 1:\n",
    "\n",
    "    df_determined_knn_minmax = model_knn_test(model_knn_minmax, x_test_minmax, y_test_minmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 4.5 - Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################################################\n",
    "# ====================================================================================================================== #\n",
    "# | ML with data STD normalization                             | | ML with data MINMAX normalization                   | #\n",
    "# ====================================================================================================================== #\n",
    "#                                                                                                                        #\n",
    "# MLPRegressor(hidden_layer_sizes=(100, 50), activation='relu', solver='adam', random_state=42)                          #\n",
    "# DecisionTreeRegressor(max_depth=100, min_samples_leaf=2, random_state = 42)                                            #\n",
    "# RandomForestRegressor(n_estimators=100, max_depth=50, min_samples_leaf=2, random_state = 42)                           #\n",
    "# KNeighborsRegressor(n_neighbors=3, leaf_size=30, weights='distance', algorithm='auto', metric='euclidean')             #\n",
    "#                                                                                                                        #\n",
    "# ====================================================================================================================== #\n",
    "# |      |                 | *MLPR* | *DTR*  | *RFR*  | *KNN*  | |                 | *MLPR* | *DTR*  | *RFR*  | *KNN*  | #\n",
    "# ---------------------------------------------------------------------------------------------------------------------- #\n",
    "# | MAE  |                 | 0.1767 | 0.2282 | 0.1692 | 0.1999 | |                 | 0.0319 | 0.0410 | 0.0304 | 0.0344 | #\n",
    "# ---------------------------------------------------------------------------------------------------------------------- #\n",
    "# | MSE  |                 | 0.1109 | 0.2035 | 0.1111 | 0.1489 | |                 | 0.0036 | 0.0065 | 0.0035 | 0.0046 | #\n",
    "# ---------------------------------------------------------------------------------------------------------------------- #\n",
    "# | RMSE |                 | 0.3331 | 0.4512 | 0.3334 | 0.3859 | |                 | 0.0605 | 0.0811 | 0.0599 | 0.0677 | #\n",
    "# ---------------------------------------------------------------------------------------------------------------------- #\n",
    "# | R2   |                 | 0.8889 | 0.7963 | 0.8887 | 0.8510 | |                 | 0.8865 | 0.7962 | 0.8887 | 0.8580 | #\n",
    "# ---------------------------------------------------------------------------------------------------------------------- #\n",
    "# | ttrn |                 | 11m28s | 48.3s  | 61m45s | 19,3s  | |                 | 13m38s | 44.7s  | 61m40s | 16,7s  | #\n",
    "# ---------------------------------------------------------------------------------------------------------------------- #\n",
    "# | ttst |                 | 18.3s  | 1.0s   | 3m4s   | 5m33s  | |                 | 5s     | 1.0s   | 4m3.6s | 4m17s  | #\n",
    "# ---------------------------------------------------------------------------------------------------------------------- #\n",
    "##########################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "#### Step 5 - Variables selection to optimize the ML models\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
