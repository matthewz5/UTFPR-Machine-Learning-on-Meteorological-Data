{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Research for the statistical analysis of meteorological data and machine learning methods for determining solar irradiation.\n",
    "\n",
    "Researcher: Matheus Henrique da Silva.\n",
    "Control and Automation Engineering Student.\n",
    "Universidade Tecnológica Federal do Paraná, Cornélio Procópio, Paraná, Brasil. \n",
    "E-mail: matheussilva.2019@alunos.utfpr.edu.br. \n",
    "ID Lattes: 5450995625966991.\n",
    "\n",
    "Supervisor: Wesley Angelino de Souza.\n",
    "Lecturer in the Graduate Program in Electrical Engineering.\n",
    "Universidade Tecnológica Federal do Paraná, Cornélio Procópio, Paraná, Brasil. \n",
    "E-mail: wesleyangelino@utfpr.edu.br. \n",
    "ID Lattes: 8594457321079718."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "#### Step 1 - data extraction and agglutination\n",
    "\n",
    "Data collected and made available by the Instituto Nacional de Meteorologia (INMET), acess on https://portal.inmet.gov.br/, in format .csv of 606 meteorological collection stations of Brazil between 2010 and 2021.\n",
    "\n",
    "Each .csv file contains 9 lines with information about the station: Nome; Codigo Estacao; Latitude; Longitude; Altitude; Situacao; Data Inicial; Data Final e Periodicidade da Medicao.\n",
    "\n",
    "In line 11, contains the name of the variables: Data Medicao; Hora Medicao; PRECIPITACAO TOTAL, HORARIO(mm); PRESSAO ATMOSFERICA AO NIVEL DA ESTACAO, HORARIA(mB); PRESSAO ATMOSFERICA REDUZIDA NIVEL DO MAR, AUT(mB); PRESSAO ATMOSFERICA MAX.NA HORA ANT. (AUT)(mB); PRESSAO ATMOSFERICA MIN. NA HORA ANT. (AUT)(mB); RADIACAO GLOBAL(Kj/mÂ²); TEMPERATURA DA CPU DA ESTACAO(Â°C); TEMPERATURA DO AR - BULBO SECO, HORARIA(Â°C); TEMPERATURA DO PONTO DE ORVALHO(Â°C); TEMPERATURA MAXIMA NA HORA ANT. (AUT)(Â°C); TEMPERATURA MINIMA NA HORA ANT. (AUT)(Â°C); TEMPERATURA ORVALHO MAX. NA HORA ANT. (AUT)(Â°C); TEMPERATURA ORVALHO MIN. NA HORA ANT. (AUT)(Â°C); TENSAO DA BATERIA DA ESTACAO(V); UMIDADE REL. MAX. NA HORA ANT. (AUT)(%); UMIDADE REL. MIN. NA HORA ANT. (AUT)(%); UMIDADE RELATIVA DO AR, HORARIA(%); VENTO, DIRECAO HORARIA (gr)(Â° (gr)); VENTO, RAJADA MAXIMA(m/s); VENTO, VELOCIDADE HORARIA(m/s).\n",
    "\n",
    "From line 12, the data collected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe with the informations of all stations\n",
    "\n",
    "df_stations = pd.DataFrame(columns =  [\"csv\", \"nome\", \"codigo\", \"latitude\", \n",
    "                                       \"longitude\", \"altitude\", \"situacao\", \"data_inicio\", \n",
    "                                       \"data_fim\", \"periodicidade\"])\n",
    "\n",
    "for file in glob.glob(\"*.csv\"):\n",
    "        \n",
    "    arquivo = file\n",
    "    file = open(file)\n",
    "\n",
    "    csv_reader = csv.reader(file)\n",
    "\n",
    "    # 1: 'nome': nome_estacao\n",
    "    row = next(csv_reader)\n",
    "    row=str(row[0])\n",
    "    tmp=row.split(\": \")\n",
    "    nome_estacao = tmp[1]\n",
    "    \n",
    "    # 2: 'codigo': codigo_estacao\n",
    "    row = next(csv_reader)\n",
    "    row=str(row[0])\n",
    "    tmp=row.split(\": \")\n",
    "    codigo_estacao = tmp[1]\n",
    "    \n",
    "    # 3: 'latitude': latitude_estacao\n",
    "    row = next(csv_reader)\n",
    "    row=str(row[0])\n",
    "    tmp=row.split(\": \")\n",
    "    latitude_estacao = tmp[1]\n",
    "    \n",
    "    # 4: 'longitude': longitude_estacao\n",
    "    row = next(csv_reader)\n",
    "    row=str(row[0])\n",
    "    tmp=row.split(\": \")\n",
    "    longitude_estacao = tmp[1]\n",
    "    \n",
    "    # 5: 'altitude': altitude_estacao\n",
    "    row = next(csv_reader)\n",
    "    row=str(row[0])\n",
    "    tmp=row.split(\": \")\n",
    "    altitude_estacao = tmp[1]\n",
    "    \n",
    "    # 5: 'situacao': situacao_estacao\n",
    "    row = next(csv_reader)\n",
    "    row=str(row[0])\n",
    "    tmp=row.split(\": \")\n",
    "    situacao_estacao = tmp[1]\n",
    "    \n",
    "    # 6: 'data_inicio': data_inicio_coleta_estacao\n",
    "    row = next(csv_reader)\n",
    "    row=str(row[0])\n",
    "    tmp=row.split(\": \")\n",
    "    data_inicio_coleta_estacao = tmp[1]\n",
    "    \n",
    "    # 7: 'data_fim': data_fim_coleta_estacao\n",
    "    row = next(csv_reader)\n",
    "    row=str(row[0])\n",
    "    tmp=row.split(\": \")\n",
    "    data_fim_coleta_estacao = tmp[1]\n",
    "\n",
    "    # 8: 'periodicidade': peridiocidade_dados_estacao\n",
    "    row = next(csv_reader)\n",
    "    row=str(row[0])\n",
    "    tmp=row.split(\": \")\n",
    "    peridiocidade_dados_estacao = tmp[1]\n",
    "    \n",
    "    new_row = {'csv': str(arquivo), \n",
    "               'nome': nome_estacao, \n",
    "               'codigo': codigo_estacao, \n",
    "               'latitude': latitude_estacao,\n",
    "               'longitude': longitude_estacao,\n",
    "               'altitude': altitude_estacao,\n",
    "               'situacao': situacao_estacao, \n",
    "               'data_inicio': data_inicio_coleta_estacao, \n",
    "               'data_fim': data_fim_coleta_estacao, \n",
    "               'periodicidade': peridiocidade_dados_estacao}\n",
    "    \n",
    "    df_stations = df_stations.append(new_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identification of data types and changes for station selection\n",
    "\n",
    "data_types = df_stations.dtypes\n",
    "\n",
    "print(data_types)\n",
    "\n",
    "df_stations['latitude'] = df_stations['latitude'].astype(float)\n",
    "df_stations['longitude'] = df_stations['longitude'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selection of the stations in Cornélio Procópio-PR region\n",
    "\n",
    "lat = -23.185038698153438\n",
    "lon = -50.647548006591066\n",
    "dist = 4\n",
    "\n",
    "lim_lat_higher = lat + dist \n",
    "lim_lat_bottom = lat - dist\n",
    "\n",
    "lon =  -50.647548006591066\n",
    "lim_lon_right = lon + dist\n",
    "lim_lon_left = lon - dist\n",
    "\n",
    "selected_stations = df_stations[((df_stations['latitude'] >= lim_lat_bottom) & (df_stations['latitude'] <= lim_lat_higher)) & \\\n",
    "                            ((df_stations['longitude'] >= lim_lon_left) & (df_stations['longitude'] <= lim_lon_right))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_stations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic map for station location\n",
    "\n",
    "map = folium.Map(location=[lat, lon], zoom_start=14)\n",
    "\n",
    "folium.Marker(location=[lat, lon], popup='CORNÉLIO PROCÓPIO').add_to(map)\n",
    "\n",
    "lenght = selected_stations.shape[0]\n",
    "\n",
    "lat_ar = np.zeros(lenght)\n",
    "lon_ar = np.zeros(lenght)\n",
    "est_ar = np.empty((lenght), dtype=object)\n",
    "\n",
    "index = selected_stations.index.to_numpy()\n",
    "\n",
    "for i in range(lenght):\n",
    "\n",
    "    lat_ar[i] = selected_stations.loc[index[i], 'latitude']\n",
    "    lon_ar[i] = selected_stations.loc[index[i], 'longitude']\n",
    "    est_ar[i] = selected_stations.loc[index[i], 'nome']\n",
    "\n",
    "    popup_station = folium.Popup(f'{est_ar[i]}\\n{lat_ar[i]},{lon_ar[i]}')\n",
    "    folium.Marker(location=[lat_ar[i], lon_ar[i]], popup=popup_station).add_to(map)\n",
    "\n",
    "map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe with the data agglutination\n",
    "\n",
    "data_loaded = 1\n",
    "\n",
    "if(data_loaded==0):\n",
    "    \n",
    "    result = pd.DataFrame()\n",
    "\n",
    "    for file in selected_stations['csv']:\n",
    "\n",
    "        df_temp = pd.read_csv(file, skiprows=10, delimiter=\";\", decimal=\",\")\n",
    "        df_temp.drop(df_temp.columns[[4,5,6,8,11,12,13,14,15,16,17,22]],axis=1, inplace = True)\n",
    "        df_temp.dropna(inplace=True)\n",
    "\n",
    "        result = pd.concat([result, df_temp])\n",
    "        \n",
    "    result.to_csv('data_agglutinated/data_selected_stations.csv') \n",
    "    \n",
    "else:\n",
    "    result = pd.read_csv('data_agglutinated/data_selected_stations.csv', delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "#### Step 2 - statistical and exploratory analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.1 - load and transformation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "data = pd.read_csv('data_agglutinated/data_selected_stations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change variables name and format of date\n",
    "\n",
    "data.drop(data.columns[0],axis=1, inplace = True)\n",
    "data.rename(columns = {'Data Medicao': 'data'}, inplace = True)\n",
    "data['data'] = pd.to_datetime(data['data'])\n",
    "data.rename(columns = {'Hora Medicao': 'hora'}, inplace = True)\n",
    "data.rename(columns = {'PRECIPITACAO TOTAL, HORARIO(mm)': 'precipitacao'}, inplace = True)\n",
    "data.rename(columns = {'PRESSAO ATMOSFERICA AO NIVEL DA ESTACAO, HORARIA(mB)': 'pressao_atm'}, inplace = True)\n",
    "data.rename(columns = {'RADIACAO GLOBAL(Kj/m²)': 'radiacao'}, inplace = True)\n",
    "data.rename(columns = {'TEMPERATURA DO AR - BULBO SECO, HORARIA(°C)': 'temperatura'}, inplace = True)\n",
    "data.rename(columns = {'TEMPERATURA DO PONTO DE ORVALHO(°C)': 'temp_orvalho'}, inplace = True)\n",
    "data.rename(columns = {'UMIDADE RELATIVA DO AR, HORARIA(%)': 'umidade'}, inplace = True)\n",
    "data.rename(columns = {'VENTO, DIRECAO HORARIA (gr)(° (gr))': 'direcao_vento'}, inplace = True)\n",
    "data.rename(columns = {'VENTO, RAJADA MAXIMA(m/s)': 'vento_maximo'}, inplace = True)\n",
    "data.rename(columns = {'VENTO, VELOCIDADE HORARIA(m/s)': 'velocidade'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of target variable 'radiacao' by 'hora'\n",
    "\n",
    "data.plot(kind=\"scatter\", x=\"hora\", y=\"radiacao\", alpha=0.1)\n",
    "\n",
    "plt.savefig('figures/scatter_radiacao_hora.png', format='png', dpi = 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the preliminary analyses, we noticed that the data are concentrated between 10 am and 10 pm\n",
    "\n",
    "However, solar radiation begins to be noticed along with sunrise ~ 6:40 am\n",
    "\n",
    "Therefore, we will shift the data by -3 hours each (due to the Brazilian time zone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_hour(df):\n",
    "\n",
    "    df['hora'] = df['hora'].replace(0, 21)\n",
    "    df['hora'] = df['hora'].replace(100, 22)\n",
    "    df['hora'] = df['hora'].replace(200, 23)\n",
    "    df['hora'] = df['hora'].replace(300, 0)\n",
    "    df['hora'] = df['hora'].replace(400, 1)\n",
    "    df['hora'] = df['hora'].replace(500, 2)\n",
    "    df['hora'] = df['hora'].replace(600, 3)\n",
    "    df['hora'] = df['hora'].replace(700, 4)\n",
    "    df['hora'] = df['hora'].replace(800, 5)\n",
    "    df['hora'] = df['hora'].replace(900, 6)\n",
    "    df['hora'] = df['hora'].replace(1000, 7)\n",
    "    df['hora'] = df['hora'].replace(1100, 8)\n",
    "    df['hora'] = df['hora'].replace(1200, 9)\n",
    "    df['hora'] = df['hora'].replace(1300, 10)\n",
    "    df['hora'] = df['hora'].replace(1400, 11)\n",
    "    df['hora'] = df['hora'].replace(1500, 12)\n",
    "    df['hora'] = df['hora'].replace(1600, 13)\n",
    "    df['hora'] = df['hora'].replace(1700, 14)\n",
    "    df['hora'] = df['hora'].replace(1800, 15)\n",
    "    df['hora'] = df['hora'].replace(1900, 16)\n",
    "    df['hora'] = df['hora'].replace(2000, 17)\n",
    "    df['hora'] = df['hora'].replace(2100, 18)\n",
    "    df['hora'] = df['hora'].replace(2200, 19)\n",
    "    df['hora'] = df['hora'].replace(2300, 20)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = adjust_hour(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data dataframe info, quantities and types\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for NULL values\n",
    "\n",
    "non_null_counts = data.count()\n",
    "\n",
    "print(non_null_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical describe\n",
    "\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.2 - data cleaning\n",
    "\n",
    "Previously, there were discrepant values ​​noticed at unconventional times, for this reason they caused the outliers to be cleaned in a different way:\n",
    "\n",
    "We take the data per hour, and remove according to the variable_target 'radiacao' the last 3.5 * standard deviation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_out(df):\n",
    "\n",
    "    dados_0 = df[df['hora'] == 0]\n",
    "    dados_1 = df[df['hora'] == 1]\n",
    "    dados_2 = df[df['hora'] == 2]\n",
    "    dados_3 = df[df['hora'] == 3]\n",
    "    dados_4 = df[df['hora'] == 4]\n",
    "    dados_5 = df[df['hora'] == 5]\n",
    "    dados_6 = df[df['hora'] == 6]\n",
    "    dados_7 = df[df['hora'] == 7]\n",
    "    dados_8 = df[df['hora'] == 8]\n",
    "    dados_9 = df[df['hora'] == 9]\n",
    "    dados_10 = df[df['hora'] == 10]\n",
    "    dados_11 = df[df['hora'] == 11]\n",
    "    dados_12 = df[df['hora'] == 12]\n",
    "    dados_13 = df[df['hora'] == 13]\n",
    "    dados_14 = df[df['hora'] == 14]\n",
    "    dados_15 = df[df['hora'] == 15]\n",
    "    dados_16 = df[df['hora'] == 16]\n",
    "    dados_17 = df[df['hora'] == 17]\n",
    "    dados_18 = df[df['hora'] == 18]\n",
    "    dados_19 = df[df['hora'] == 19]\n",
    "    dados_20 = df[df['hora'] == 20]\n",
    "    dados_21 = df[df['hora'] == 21]\n",
    "    dados_22 = df[df['hora'] == 22]\n",
    "    dados_23 = df[df['hora'] == 23]\n",
    "\n",
    "    data_hour = [dados_0, dados_1, dados_2, dados_3, dados_4, dados_5, dados_6, dados_7, dados_8, dados_9, dados_10, dados_11, dados_12, dados_13, dados_14, dados_15, dados_16, dados_17, dados_18, dados_19, dados_20, dados_21, dados_22, dados_23]\n",
    "\n",
    "    data_filtered = pd.DataFrame()\n",
    "\n",
    "    for i in data_hour:\n",
    "\n",
    "        mean_radiacao = np.mean(i['radiacao'])\n",
    "        std_radiacao = np.std(i['radiacao'])\n",
    "\n",
    "        lower = mean_radiacao - 3.5 * std_radiacao\n",
    "        upper = mean_radiacao + 3.5 * std_radiacao\n",
    "\n",
    "        y = i.query('radiacao <= @upper')\n",
    "\n",
    "        data_filtered = pd.concat([data_filtered, y])\n",
    "\n",
    "    return data_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = remove_out(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of target variable 'radiacao' by 'hora' with filtered data\n",
    "\n",
    "data.plot(kind=\"scatter\", x=\"hora\", y=\"radiacao\", alpha=0.1)\n",
    "\n",
    "plt.savefig('figures/scatter_radiacao_hora_filtered.png', format='png', dpi = 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2.3 - statistical analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical describe with filtered data\n",
    "\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of the variables\n",
    "\n",
    "def histogram_variables(df, filename=None):\n",
    "\n",
    "    df.hist(bins=50, figsize=(20,15))\n",
    "\n",
    "    if filename is not None:\n",
    "        plt.savefig(filename, format='pdf', bbox_inches='tight')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_variables(data, 'figures/histogram_variables.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter of the target variable 'radiacao' with all variables\n",
    "\n",
    "def scatter_variables(df, variable, filename=None):\n",
    "\n",
    "    variables = df.columns.drop(variable)\n",
    "\n",
    "    data = df[[variable] + list(variables)]\n",
    "\n",
    "    sns.pairplot(data, x_vars=variables, y_vars=variable, height=2.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if filename is not None:\n",
    "        plt.savefig(filename, dpi=500, bbox_inches='tight')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_variables(data, 'radiacao', 'figures/scatter_variables.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skewness values\n",
    "\n",
    "data.skew(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pearson correlation\n",
    "\n",
    "def pearson_correlation_map(df, filename=None):\n",
    "\n",
    "    plt.figure(figsize = (12,8))\n",
    "\n",
    "    sns.heatmap(df.corr(), annot = True, fmt = '.4f', cmap = 'Reds', vmax = .99, vmin = -0.60)\n",
    "\n",
    "    if filename is not None:\n",
    "        plt.savefig(filename, format='pdf', bbox_inches='tight')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson_correlation_map(data, 'figures/pearson_correlation_map.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "#### Step 3 - data normalization\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
